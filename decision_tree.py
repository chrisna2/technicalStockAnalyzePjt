"""
>> 결정트리모델 (Decision Tree Model) 
 - 결정 트리 모델은 데이터 분류 및 회귀 분석을 위한 기계학습 모델
 - 의사결정을 나무의 가지처럼 분기하는 구조로 시각화 하여 데이터를 분석함
 - 루트노드(Root Node)에서 시작하여, 각 노드(Node)에서 조건에 따라 데이터를 분할 하면서 리프노드(Leaf Node)까지 도달
 
>> 결정 트리의 구성 요소
 - 루트노드(Root Node) : 트리의 최상단 노드로, 전체 데이터를 포함함
 - 내부노드(Internal Node) : 하나 이상의 조건(feature)에 따라 데이터를 분할 하는 중간노드
 - 가지(branch) : 노드 간의 연결을 나타내며, 조건에 따른 데이터의 흐름을 표현
 - 리프노드(Leaf Node) : 더 이상 분할되지 않는 최종 노드로, 예측 결과를 나타냄
 
>> 경정트리의 작동방식
 - 분류(Classification) : 데이터를 여러 클래스(범주)로 분류하는 작업
   - 카테고리형 데이터의 경우. 분할기준으로 지니불순도나 엔트로피와 같은 척도를 사용
 - 회귀(Regression) : 연속적인 값을 예측하는 작업
   - 연속형 데이터의 경우, 평균 제곱 오차(MSE)와 같은 척도를 사용하여 분할 기준을 설정

>> 결정트리의 장점
 1) 해석용이성
   - 결정트리는 시각적으로 이해하기 쉬워, 비전문가도 모델의 의사결정 과정을 쉽게 파악할 수 있음
 2) 데이터 전처리 필요성 낮음
   - 스케일링이나 정규화와 같은 데이터 전처리 작업이 거이 필요 없음
 3) 다양한 데이터 처리
   - 범주형 및 연속형 데이터를 모두 처리할 수 있음

>> 결정트리의 단점
    1) 과적합(Overfitting)
    - 트리가 너무 깊어지면 훈련 데이터에 과적합될 수 있어, 일반화 성능이 떨어질 수 있음
    - 새로운 데이터에 대한 일반화 성능이 떨어짐
    - 이를 방지하기 위해 가지치기(pruning) 기법이나 최대 깊이 제한 등을 사용할 수 있음
    2) 불안정성
    - 작은 데이터 변화에도 트리 구조가 크게 변할 수 있어, 모델의 안정성이 떨어질 수 있음
    - 이를 완화하기 위해 앙상블 기법(예: 랜덤 포레스트)을 사용할 수 있음
    3) 편향(Bias)
    - 특정 특성이 다른 특성보다 더 많이 선택되는 경향이 있어, 편향된 모델이 될 수 있음
    - 이를 완화하기 위해 특성 중요도(feature importance)를 고려한 특성 선택 방법을 사용할 수 있음

"""

from sklearn.tree import DecisionTreeRegressor # 결정트리 회귀 모델
from sklearn.model_selection import train_test_split # 데이터 분할
import FinanceDataReader as fdr # 금융 데이터 수집 라이브러리
import pandas as pd

if __name__ == '__main__':
    # 삼성전자 주가 데이터 수집
    df = fdr.DataReader('005930').dropna()  # 삼성전자, 결측값 제거

    x = [] #
    y = [] #

    for i in range(len(df) - 1):
        a = df.iloc[i].to_numpy() # i번째 행의 모든 열 데이터를 numpy 배열로 변환
        b = df.iloc[i+1]["Close"] # i+1번째 행의 "Close" 열 데이터 (다음날 종가)
        x.append(a)
        y.append(b)

    # 학습 데이터와 테스트 데이터로 분할 (기본 비율 75% 학습, 25% 테스트)
    train_x, test_x, train_y, test_y = train_test_split(x, y)

    model = DecisionTreeRegressor() # 결정트리 회귀 모델 객체 생성
    model.fit(train_x, train_y) # 모델 학습

    score = model.score(test_x, test_y) # 모델 평가 (R^2 점수)
    today_data = df.iloc[-1].to_numpy() # 가장 최근(오늘)의 주가 데이터
    pred = model.predict([today_data])[0] # 오늘 데이터를 기반으로 내일 종가 예측

    date = str(df.iloc[-1].name + pd.Timedelta(days=1))[:10]  # 내일 날짜 계산

    print(f"삼성전자 {date} 일자 예상 가격 : {pred:,.0f}원 입니다. 모델 정확도는 {score:.4f}입니다. 오차범위는 {(1-score)*100:,.2f}% 입니다.")

    """
     >> KNN vs 결정트리 비교
        1) 모델 구조
        - KNN : 인스턴스 기반 학습, 훈련 데이터 전체를 저장하고 예측 시점에 계산
        - 결정트리 : 트리 구조로 데이터를 분할하여 예측, 모델 자체가 학습됨
        
        2) 예측 방식
        - KNN : 새로운 데이터 포인트와 훈련 데이터 간의 거리를 계산하여 가장 가까운 K개의 이웃을 참조
        - 결정트리 : 트리를 따라 조건에 맞게 데이터를 분할하여 리프 노드에서 예측값 도출
        
        3) 계산 복잡도
        - KNN : 예측 시점에 모든 훈련 데이터와의 거리를 계산해야 하므로, 데이터 양이 많을수록 예측 시간이 길어짐
        - 결정트리 : 예측 시점에 트리를 따라가므로, 예측 속도가 빠름
        
        4) 데이터 전처리
        - KNN : 특성 스케일링(정규화, 표준화 등)이 필요함, 거리 계산에 민감
        - 결정트리 : 특성 스케일링이 필요없음, 범주형 및 연속형 데이터를 모두 처리 가능
       
        5) 과적합 경향
        - KNN : K값이 작을 때 과적합될 수 있음, K값이 클 때는 과소적합될 수 있음
        - 결정트리 : 트리가 너무 깊어지면 과적합될 수 있음, 가지치기(pruning) 기법으로 완화 가능
        
        6) 해석 용이성
        - KNN : 모델이 직관적이지만, 예측 과정이 복잡하여 해석이 어려울 수 있음
        - 결정트리 : 트리 구조가 시각적으로 이해하기 쉬워, 모델의 의사결정 과정을 쉽게 파악 가능
        
        7) 메모리 사용
        - KNN : 모든 훈련 데이터를 메모리에 저장해야 하므로, 데이터 양이 많을수록 메모리 사용량이 증가
        - 결정트리 : 모델 자체만 저장하면 되므로, 메모리 사용량이 상대적으로 적음
        
        8) 적용 분야
        - KNN : 소규모 데이터셋, 비선형 분포, 분류 및 회귀 문제에 모두 사용
        - 결정트리 : 대규모 데이터셋, 해석이 중요한 경우, 분류 및 회귀 문제에 모두 사용
        
     >> KNN보다 결정트리가 오차범위가 낮은 이유
        - KNN은 다양한 데이터 전처리 및 표준화 작업을 해줘야 하지만 결정트리는 그런 작업이 필요없음
        - 결정트리는 트리 구조로 데이터를 분할하여 예측하므로, 데이터의 복잡한 패턴을 더 잘 포착할 수 있음
    """


