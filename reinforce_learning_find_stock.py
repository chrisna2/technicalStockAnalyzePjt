"""
>> 강화 학습 (Reinforcement Learning, RL)
 - 강화 학습은 에이전트가 환경과 상호작용하면서 보상을 최대화하는 행동을 학습하는 머신러닝의 한 분야입니다.

>> 강화 학습 용어
    - 에이전트 (Agent): 환경과 상호작용하는 주체입니다. 예를 들어, 게임에서 플레이어 캐릭터가 에이전트가 될 수 있습니다.
    - 환경 (Environment): 에이전트가 상호작용하는 세계입니다. 예를 들어, 게임의 맵이나 시뮬레이션 환경이 될 수 있습니다.
    - 상태 (State): 환경의 현재 상황을 나타내는 정보입니다. 예를 들어, 게임에서 플레이어의 위치나 점수가 상태가 될 수 있습니다.
    - 행동 (Action): 에이전트가 환경에서 취할 수 있는 선택입니다. 예를 들어, 게임에서 이동, 공격, 점프 등이 행동이 될 수 있습니다.
    - 보상 (Reward): 에이전트가 특정 행동을 취한 후 환경으로부터 받는 피드백입니다. 보상은 긍정적일 수도 있고 부정적일 수도 있습니다.
    - 정책 (Policy): 에이전트가 상태에 따라 어떤 행동을 취할지 결정하는 전략입니다.

>> 강화 학습의 주요 알고리즘
    - Q-러닝 (Q-Learning): 에이전트가 상태-행동 쌍에 대한 가치를 학습하여 최적의 정책을 찾는 방법입니다.
    - 딥 Q-네트워크 (Deep Q-Network, DQN): Q-러닝에 딥러닝을 결합한 방법으로, 복잡한 상태 공간에서도 효과적으로 학습할 수 있습니다.
    - 정책 경사 (Policy Gradient): 에이전트가 직접 정책을 최적화하는 방법으로, 확률적 정책을 사용할 수 있습니다.
    - 액터-크리틱 (Actor-Critic): 정책과 가치 함수를 동시에 학습하는 방법으로, 안정적인 학습을 제공합니다.

>> 강화 학습 : 엡실론 - 그리디 (Epsilon-Greedy) 전략
 - 탐험과 활용 :
    - 탐험 (Exploration) : 새로운 행동을 시도하여 더 나은 보상을 찾는 과정입니다.
        - 새로운 상태와 행동을 시도하여 더 많은 정보를 수집
    - 활용 (Exploitation) : 이미 알고 있는 정보를 바탕으로 최적의 행동을 선택하는 과정입니다.
        - 현재까지 학습한 정ㅂ를 기반으로 최저의 행동을 선택하여 보상을 극대화
    - 탐험을 많이 하면 : 새로운 정보를 많이 얻을 수 있지만, 단기적인 보상이 낮아질 수 있습니다.
    - 활용을 많이 하면 : 단기적인 보상이 높아질 수 있지만, 새로운 정보를 놓칠 수 있습니다.
    - 앱실론-그리드알고리즘은 탐험과 활용의 균형을 맞추기 위한 방법
  - 앱실론-그리디 알고리즘 작동 방식
    - 앱실론 값 : 탑험할 확률 (0과 1 사이의 값)
    - 랜덤 행동 선택 : 엡실론 값의 확율로 에이전트는 무작위로 행동을 선택하여 탐험을 수행함
      - 예를 들어, 엡실론이 0.1이라면, 10%의 확률로 무작위 행동을 선택
    - 최적 행동(활용) 선택 : 엡실론 값의 (1 - 엡실론) 확률로 에이전트는 현재까지 학습한 정책에 따라 최적의 행동을 선택하여 활용을 수행
      - 현재 학습된 정책에서 가장 보상이 높을 것으로 예상되는 행동을 선택
      - 엡실론 값이 높을수록 탐험이 많아지고, 낮을수록 활용이 많아짐
   - 알고리즘 과정
        (1) 초기화 : Q-테이블 또는 정책을 초기화하고, 엡실론 값을 설정
            - 예를 들어 엡실로 값이 0.4이라면, 40%의 확률로 탐험을 수행, 60%의 확률로 활용을 수행
        (2) 행동 선택 :
            - 엡실론 확률로 무작위 행동을 선택 (탐험)
            - 개선된 엡실론 확률 (1 - 엡실론) 현재 상태에서 최적의 행동을 선택 (활용)
        (3) 보상 수집 :
            - 선택한 행동을 환경에서 수행하고, 그에 따른 보상을 관찰
        (4) Q-값 또는 정책 업데이트 :
            - Q-값 또는 정책을 업데이트 함
        (5) 반복 : (2)~(4) 과정을 여러 에피소드에 걸쳐 반복


"""

import FinanceDataReader as fdr
import pandas as pd
from tqdm import tqdm  # 진행 상황 표시를 위한 라이브러리

if __name__ == '__main__':

    count = 0
    total_count = 0

    # KRX 종목 데이터 가져오기
    stocks = fdr.StockListing('KRX').head(1000) # 상위 100개 종목만 사용, 사유 속도 이슈

    # 결과 저장 리스트
    rates = []

    print("상승장 추세의 국내 주식들 ...")
    # 각 종목에 대해 데이터 처리 및 예측 수행 (진행 상황 표시)
    for s in range(len(stocks)):
        code = stocks.iloc[s]['Code']  # 종목 코드
        name = stocks.iloc[s]['Name']  # 종목 이름

        df = fdr.DataReader("NAVER:" + code).dropna()

        Q = 0

        for i in range(len(df)-1):
            # 다음날 종가가 오늘 종가의 5% 이내면 상승으로 간주
            if df.iloc[i]['Close'] * 1.05 < df.iloc[i+1]['Close']:
                reward = 1
            else:
                reward = 0

            if Q > 0.9:
                if reward:
                    count += 1
                total_count += 1
                rates.append(count/total_count)

            Q += (reward - Q) * 0.8

        if Q > 0.9 :
            print(f"{name}({code}) => {Q} ({count}/{total_count})")